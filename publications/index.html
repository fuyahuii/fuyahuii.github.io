<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>Yahui Fu | publications</title>
  <meta name="description" content="This is the personal website of Yahui Fu">

  <!-- Fonts and Icons -->
  <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons" />

  <!-- CSS Files -->
  <link rel="stylesheet" href="/assets/css/all.min.css">
  <link rel="stylesheet" href="/assets/css/academicons.min.css">
  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="canonical" href="/publications/">
</head>
<body>
  <!-- Header -->
  <nav id="navbar" class="navbar fixed-top navbar-expand-md grey lighten-5 z-depth-1 navbar-light">
    <div class="container-fluid p-0">
      
        <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Yahui</span> Fu</a>
      
      <button class="navbar-toggler ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
            
              <li class="nav-item ">
                  <a class="nav-link" href="/cv/">
                    curriculum vitae
                    
                  </a>
              </li>
            
          
            
          
            
              <!-- <li class="nav-item ">
                  <a class="nav-link" href="/projects/">
                    projects
                    
                  </a>
              </li> -->
            
          
            
              <li class="nav-item navbar-active font-weight-bold">
                  <a class="nav-link" href="/publications/">
                    publications
                    
                      <span class="sr-only">(current)</span>
                    
                  </a>
              </li>
            
          
            
              <!-- <li class="nav-item ">
                  <a class="nav-link" href="/teaching/">
                    teaching
                    
                  </a>
              </li> -->
            
          
            
          
        </ul>
      </div>
    </div>
  </nav>

  <!-- Scrolling Progress Bar -->
  <progress id="progress" value="0">
    <div class="progress-container">
      <span class="progress-bar"></span>
    </div>
  </progress>

  <!-- Content -->
  <div class="content">
    
  <h1>publications</h1>
  <!-- <h6><nobr><em>*</em></nobr> denotes equal contribution and joint lead authorship.</h6> -->


<p><br /></p>

<div class="row m-0 p-0" style="border-top: 1px solid #ddd; flex-direction: row-reverse;"> 
  <!-- #ddd light grey -->
    <div class="col-sm-1 mt-2 p-0 pr-1">
      <h3 class="bibliography-year">2025</h3>
    </div>
    <div class="col-sm-11 p-0">
      <ol class="bibliography">
        
                <li><div class="row m-0 mt-3 p-0">
          <div class="col-sm-1 p-0 abbr">
            
              
                <a class="badge font-weight-bold light-green darken-1 align-middle" style="width: 65px;font-size: 0.75em;" href="https://2025.aaclnet.org/" target="_blank">
                  \small IJCNLP-AACL 
                </a>
         
          </div>
          <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
            
            <div id="scaling_up_llm" class="col p-0">
              <h5 class="title mb-0">Minority-Aware Satisfaction Estimation in Dialogue Systems via Preference-Adaptive Reinforcement Learning</h5>
              <div class="author">
                          <nobr><em>Yahui Fu</em>,</nobr>
                          <nobr><a href="https://zihaurpang.github.io/" target="_blank">Zi Haur Pang</a>,</nobr>
                          <nobr><a href="http://sap.ist.i.kyoto-u.ac.jp/members/kawahara/" target="_blank">Tatsuya Kawahara</a>.</nobr>                 
              </div>
        
              <div>
                <p class="periodical font-italic">
                  
                  IJCNLP-AACL, Main Conference           
                </p>
              </div>
            
              <div class="col p-0">
                
                  <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#scaling_up_llm-abstract" role="button" aria-expanded="false" aria-controls="scaling_up_llm-abstract">Abstract</a>     
                  <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="https://arxiv.org/pdf/2505.13978" target="_blank">PDF</a>  -->
                  <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/2025.coling-demos.14.pdf" target="_blank">PDF</a>  -->
                  <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="https://github.com/fuyahuii/StyEmp" target="_blank">Code</a> -->
                  <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="" target="_blank">Slides</a>  -->
              </div>
              
              <div class="col mt-2 p-0">
                <div id="scaling_up_llm-abstract" class="collapse">
                  <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                    Given the inherent subjectivity of user satisfaction in dialogue systems, minority users may assign different satisfaction ratings than majority users for system responses due to varying intents and preferences. Existing methods for aligning language models with human preferences primarily focus on training universal systems that minimize controversy while overlooking the need for user-specific adaptation. We propose a unified framework that models both individual-specific and group-level preferences for user satisfaction estimation. First, we introduce Chain-of-Personalized-Reasoning (CoPeR) to capture individual preferences through interpretable reasoning chains. Second, to learn group preferences, we propose an EM-based Majority-Minority Preference-Aware Clustering (M<sup>2</sup>PC) algorithm that discovers distinct user groups without supervision. Finally, we integrate these components into a preference-adaptive reinforcement learning framework (PAda-PPO) that jointly optimizes alignment with both individual and group preferences. 
Experiments on the Emotional Support Conversation dataset show consistent improvement in user satisfaction estimation.
                </div>
              </div>
              
            </div>
          </div>
        </div>
        </li>

        <li><div class="row m-0 mt-3 p-0">
          <div class="col-sm-1 p-0 abbr">
            
              
                <a class="badge font-weight-bold light-green darken-1 align-middle" style="width: 65px;" href="https://arxiv.org/abs/2505.13978" target="_blank">
                  Preprint
                </a>
         
          </div>
          <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
            
            <div id="scaling_up_llm" class="col p-0">
              <h5 class="title mb-0">Bridging Speech Emotion Recognition and Personality: Dataset and Temporal Interaction Condition Network</h5>
              <div class="author">
                          <nobr><a href="https://gitgaviny.github.io/" target="_blank">Yuan Gao</a>,</nobr>
                          <nobr><a href="https://sites.google.com/view/hshi-speech" target="_blank"> Hao Shi</a>,</nobr>
                          <nobr><em>Yahui Fu</em>,</nobr>
                          <nobr><a href="https://researchmap.jp/chu/?lang=english" target="_blank">Chenhui Chu</a>,</nobr>                 
                          <nobr><a href="http://sap.ist.i.kyoto-u.ac.jp/members/kawahara/" target="_blank">Tatsuya Kawahara</a>.</nobr>                 
              </div>
        
              <div>
                <p class="periodical font-italic">
                  
                  arXiv preprint arXiv:2505.13978                  
                </p>
              </div>
            
              <div class="col p-0">
                
                  <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#scaling_up_llm-abstract" role="button" aria-expanded="false" aria-controls="scaling_up_llm-abstract">Abstract</a>     
                  <a class="badge grey waves-effect font-weight-light mr-1" href="https://arxiv.org/pdf/2505.13978" target="_blank">PDF</a> 
                  <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/2025.coling-demos.14.pdf" target="_blank">PDF</a>  -->
                  <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="https://github.com/fuyahuii/StyEmp" target="_blank">Code</a> -->
                  <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="" target="_blank">Slides</a>  -->
              </div>
              
              <div class="col mt-2 p-0">
                <div id="scaling_up_llm-abstract" class="collapse">
                  <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                    This study investigates the interaction between personality traits and emotional expression, exploring how personality information can improve speech emotion recognition (SER). We collected personality annotation for the IEMOCAP dataset, and the statistical analysis identified significant correlations between personality traits and emotional expressions. To extract finegrained personality features, we propose a temporal interaction condition network (TICN), in which personality features are integrated with Hubert-based acoustic features for SER. Experiments show that incorporating ground-truth personality traits significantly enhances valence recognition, improving the concordance correlation coefficient (CCC) from 0.698 to 0.785 compared to the baseline without personality information. For practical applications in dialogue systems where personality information about the user is unavailable, we develop a front-end module of automatic personality recognition. Using these automatically predicted traits as inputs to our proposed TICN model, we achieve a CCC of 0.776 for valence recognition, representing an 11.17% relative improvement over the baseline. These findings confirm the effectiveness of personality-aware SER and provide a solid foundation for further exploration in personality-aware speech processing applications.                  </div>
                </div>
              </div>
              
            </div>
          </div>
        </div>
        </li>

        <li><div class="row m-0 mt-3 p-0">
          <div class="col-sm-1 p-0 abbr">
            
              
                <a class="badge font-weight-bold light-green darken-1 align-middle" style="width: 65px;" href="https://2025.sigdial.org/" target="_blank">
                  SIGdial
                </a>
         
          </div>
          <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
            
            <div id="scaling_up_llm" class="col p-0">
              <h5 class="title mb-0">Prompt-Guided Turn-Taking Prediction</h5>
              <div class="author">
                          <nobr><a href="http://www.sap.ist.i.kyoto-u.ac.jp/members/inoue/" target="_blank">Koji Inoue</a>,</nobr>
                          <nobr><a href="https://mikeyelmers.github.io/" target="_blank"> Mikey Elmers</a>,</nobr>
                          <nobr><em>Yahui Fu</em>,</nobr>
                          <nobr><a href="https://zihaurpang.github.io/" target="_blank">Zi Haur Pang</a>,</nobr>
                          <nobr><a href="http://sap.ist.i.kyoto-u.ac.jp/members/lala/" target="_blank">Divesh Lala</a>,</nobr> 
                          <nobr><a href="http://sap.ist.i.kyoto-u.ac.jp/members/ochi/" target="_blank">Keiko Ochi</a>,</nobr>                   
                          <nobr><a href="http://sap.ist.i.kyoto-u.ac.jp/members/kawahara/" target="_blank">Tatsuya Kawahara</a>.</nobr>                 
              </div>
        
              <div>
                <p class="periodical font-italic">
                  
                  In Proceedings of the 25th Annual Meeting of the Special Interest Group on Discourse and Dialogue. 
                  
                </p>
              </div>
            
              <div class="col p-0">
                
                  <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#scaling_up_llm-abstract" role="button" aria-expanded="false" aria-controls="scaling_up_llm-abstract">Abstract</a>     
                  <a class="badge grey waves-effect font-weight-light mr-1" href="https://arxiv.org/pdf/2506.21191" target="_blank">PDF</a> 
                  <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="https://github.com/fuyahuii/StyEmp" target="_blank">Code</a> -->
                  <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="" target="_blank">Slides</a>  -->
              </div>
              
              <div class="col mt-2 p-0">
                <div id="scaling_up_llm-abstract" class="collapse">
                  <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                    Turn-taking prediction models are essential components in spoken dialogue systems and conversational robots. Recent approaches leverage transformer-based architectures to predict speech activity continuously and in real-time. In this study, we propose a novel model that enables turn-taking prediction to be dynamically controlled via textual prompts. This approach allows intuitive and explicit control through instructions such as "faster" or "calmer" adapting dynamically to conversational partners and contexts. The proposed model builds upon a transformer-based voice activity projection (VAP) model, incorporating textual prompt embeddings into both channel-wise transformers and a cross-channel transformer. We evaluated the feasibility of our approach using over 950 hours of human-human spoken dialogue data. Since textual prompt data for the proposed approach was not available in existing datasets, we utilized a large language model (LLM) to generate synthetic prompt sentences. Experimental results demonstrated that the proposed model improved prediction accuracy and effectively varied turn-taking timing behaviors according to the textual prompts.
                  </div>
                </div>
              </div>
              
            </div>
          </div>
        </div>
        </li>

        <li><div class="row m-0 mt-3 p-0">
          <div class="col-sm-1 p-0 abbr">
            
              
                <a class="badge font-weight-bold light-green darken-1 align-middle" style="width: 65px;" href="https://chi2025.acm.org/" target="_blank">
                  CHI EA
                </a>
         
          </div>
          <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
            
            <div id="scaling_up_llm" class="col p-0">
              <h5 class="title mb-0">Does the Appearance of Autonomous Conversational Robots Affect User Spoken Behaviors in Real-World Conference Interactions?</h5>
              <div class="author">
                          <nobr><a href="https://zihaurpang.github.io/" target="_blank">Zi Haur Pang</a>,</nobr>
                          <nobr><em>Yahui Fu</em>,</nobr>
                          <nobr><a href="http://sap.ist.i.kyoto-u.ac.jp/members/lala/" target="_blank">Divesh Lala</a>,</nobr>
                          <nobr><a href="https://mikeyelmers.github.io/" target="_blank"> Mikey Elmers</a>,</nobr>
                          <nobr><a href="http://www.sap.ist.i.kyoto-u.ac.jp/members/inoue/" target="_blank">Koji Inoue</a>,</nobr>
                          <nobr><a href="http://sap.ist.i.kyoto-u.ac.jp/members/kawahara/" target="_blank">Tatsuya Kawahara</a>.</nobr>  

                
              </div>
        
              <div>
                <p class="periodical font-italic">
                  
                  CHI EA '25: Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems
                  
                </p>
              </div>
            
              <div class="col p-0">
                
                  <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#scaling_up_llm-abstract" role="button" aria-expanded="false" aria-controls="scaling_up_llm-abstract">Abstract</a>     
                  <a class="badge grey waves-effect font-weight-light mr-1" href="https://dl.acm.org/doi/full/10.1145/3706599.3720179" target="_blank">PDF</a> 
                  <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="https://www.youtube.com/watch?v=jCuw9g99KuE" target="_blank">Demo</a>  -->
                  <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="https://github.com/fuyahuii/StyEmp" target="_blank">Code</a> -->
                  <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="" target="_blank">Slides</a>  -->
              </div>
              
              <div class="col mt-2 p-0">
                <div id="scaling_up_llm-abstract" class="collapse">
                  <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                    We investigate the impact of robot appearance on users’ spoken behavior during real-world interactions by comparing a human-like android, ERICA, with a less anthropomorphic humanoid, TELECO. Analyzing data from 42 participants at SIGDIAL 2024, we extracted linguistic features such as disfluencies and syntactic complexity from conversation transcripts. The results showed moderate effect sizes, suggesting that participants produced fewer disfluencies and employed more complex syntax when interacting with ERICA. Further analysis involving training classification models like Naïve Bayes, which achieved an F1-score of 71.60%, and conducting feature importance analysis, highlighted the significant role of disfluencies and syntactic complexity in interactions with robots of varying human-like appearances. Discussing these findings within the frameworks of cognitive load and Communication Accommodation Theory, we conclude that designing robots to elicit more structured and fluent user speech can enhance their communicative alignment with humans.
                  </div>
                </div>
              </div>
              
            </div>
          </div>
        </div>
        </li>

        <li><div class="row m-0 mt-3 p-0">
          <div class="col-sm-1 p-0 abbr">
            
              
                <a class="badge font-weight-bold light-green darken-1 align-middle" style="width: 65px;" href="https://coling2025.org/" target="_blank">
                  COLING
                </a>
         
          </div>
          <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
            
            <div id="scaling_up_llm" class="col p-0">
              <h5 class="title mb-0">Human-Like Embodied AI Interviewer: Employing Android ERICA in Real International Conference</h5>
              <div class="author">
                          <nobr><a href="https://zihaurpang.github.io/" target="_blank">Zi Haur Pang</a>,</nobr>
                          <nobr><em>Yahui Fu</em>,</nobr>
                          <nobr><a href="http://sap.ist.i.kyoto-u.ac.jp/members/lala/" target="_blank">Divesh Lala</a>,</nobr>
                          <nobr><a href="https://mikeyelmers.github.io/" target="_blank"> Mikey Elmers</a>,</nobr>
                          <nobr><a href="http://www.sap.ist.i.kyoto-u.ac.jp/members/inoue/" target="_blank">Koji Inoue</a>,</nobr>
                          <nobr><a href="http://sap.ist.i.kyoto-u.ac.jp/members/kawahara/" target="_blank">Tatsuya Kawahara</a>.</nobr>  

                
              </div>
        
              <div>
                <p class="periodical font-italic">
                  
                  In Proceedings of the 31st International Conference on Computational Linguistics: System Demonstrations
                  
                </p>
              </div>
            
              <div class="col p-0">
                
                  <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#scaling_up_llm-abstract" role="button" aria-expanded="false" aria-controls="scaling_up_llm-abstract">Abstract</a>     
                  <a class="badge grey waves-effect font-weight-light mr-1" href="https://aclanthology.org/2025.coling-demos.14/" target="_blank">PDF</a> 
                  <a class="badge grey waves-effect font-weight-light mr-1" href="https://www.youtube.com/watch?v=jCuw9g99KuE" target="_blank">Demo</a> 
                  <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="https://github.com/fuyahuii/StyEmp" target="_blank">Code</a> -->
                  <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="" target="_blank">Slides</a>  -->
              </div>
              
              <div class="col mt-2 p-0">
                <div id="scaling_up_llm-abstract" class="collapse">
                  <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                    This paper introduces the human-like embodied AI interviewer which integrates android
                    robots equipped with advanced conversational
                    capabilities, including attentive listening, conversational repairs, and user fluency adaptation. Moreover, it can analyze and present
                    results post-interview. We conducted a realworld case study at SIGDIAL 2024 with 42
                    participants, of whom 69% reported positive
                    experiences. This study demonstrated the system’s effectiveness in conducting interviews
                    just like a human and marked the first employment of such a system at an international conference. The demonstration video is available
                    at https://youtu.be/jCuw9g99KuE.
                  </div>
                </div>
              </div>
              
            </div>
          </div>
        </div>
        </li>
</ol>
</div>
</div>




<div class="row m-0 p-0" style="border-top: 1px solid #ddd; flex-direction: row-reverse;"> 
  <!-- #ddd light grey -->
    <div class="col-sm-1 mt-2 p-0 pr-1">
      <h3 class="bibliography-year">2024</h3>
    </div>
    <div class="col-sm-11 p-0">
      <ol class="bibliography">

        <li><div class="row m-0 mt-3 p-0">
          <div class="col-sm-1 p-0 abbr">
            
              
                <a class="badge font-weight-bold light-green darken-1 align-middle" style="width: 65px;" href="https://2024.sigdial.org/" target="_blank">
                  SIGdial
                </a>
         
          </div>
          <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
            
            <div id="scaling_up_llm" class="col p-0">
              <h5 class="title mb-0">StyEmp: Stylizing Empathetic Response Generation via Multi-Grained Prefix Encoder and Personality Reinforcement</h5>
              <div class="author">
                          <nobr><em>Yahui Fu</em>,</nobr>
                          <nobr><a href="https://researchmap.jp/chu/?lang=english" target="_blank">Chenhui Chu</a>,</nobr>
                        <nobr><a href="http://sap.ist.i.kyoto-u.ac.jp/members/kawahara/" target="_blank">Tatsuya Kawahara</a>.</nobr>  
                
              </div>
        
              <div>
                <p class="periodical font-italic">
                  
                  In Proceedings of the 25th Annual Meeting of the Special Interest Group on Discourse and Dialogue. 
                  
                </p>
              </div>
            
              <div class="col p-0">
                
                  <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#scaling_up_llm-abstract" role="button" aria-expanded="false" aria-controls="scaling_up_llm-abstract">Abstract</a>     
                  <a class="badge grey waves-effect font-weight-light mr-1" href="https://aclanthology.org/2024.sigdial-1.15/" target="_blank">PDF</a> 
                  <a class="badge grey waves-effect font-weight-light mr-1" href="https://github.com/fuyahuii/StyEmp" target="_blank">Code</a>
                  <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="" target="_blank">Slides</a>  -->
              </div>
              
              <div class="col mt-2 p-0">
                <div id="scaling_up_llm-abstract" class="collapse">
                  <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                    Recent approaches for empathetic response generation mainly focus on emotional resonance and user understanding, without considering the system's personality. Consistent personality is evident in real human expression
                    and is important for creating trustworthy systems. To address this problem, we propose StyEmp, which aims to stylize the empathetic response generation with a consistent personality. Specifically, it incorporates a multi-grained
                    prefix mechanism designed to capture the intricate relationship between a system's personality and its empathetic expressions. Furthermore, we introduce a personality reinforcement module that leverages contrastive learning to
                    calibrate the generation model, ensuring that responses are both empathetic and reflective of a distinct personality. Automatic and human evaluations on the EMPATHETICDIALOGUES benchmark show that StyEmp outperforms competitive baselines in terms of both
                    empathy and personality expressions.
                  </div>
                </div>
              </div>
              
            </div>
          </div>
        </div>
        </li>

<li><div class="row m-0 mt-3 p-0">
  <div class="col-sm-1 p-0 abbr">
    
      
        <a class="badge font-weight-bold light-green darken-1 align-middle" style="width: 65px;" href="https://sites.google.com/grp.riken.jp/iwsds2024" target="_blank">
          IWSDS
        </a>
 
  </div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="scaling_up_llm" class="col p-0">
      <h5 class="title mb-0">Enhancing Personality Recognition in Dialogue by Data Augmentation and Heterogeneous Conversational Graph Networks</h5>
      <div class="author">
                  <nobr><em>Yahui Fu</em>,</nobr>
                  <nobr><a href="https://shyyhs.github.io/" target="_blank">Haiyue Song</a>,</nobr>
                  <nobr><a href="https://zhaoting.github.io/" target="_blank">Tianyu Zhao</a>,</nobr>
                <nobr><a href="http://sap.ist.i.kyoto-u.ac.jp/members/kawahara/" target="_blank">Tatsuya Kawahara</a>.</nobr>  
        
      </div>

      <div>
        <p class="periodical font-italic">
          
            In The 14th International Workshop on Spoken Dialogue Systems Technology, Sapporo, Japan. (Oral)
          
        </p>
      </div>
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#scaling_up_llm-abstract" role="button" aria-expanded="false" aria-controls="scaling_up_llm-abstract">Abstract</a>     
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://arxiv.org/abs/2401.05871" target="_blank">PDF</a> 
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://github.com/fuyahuii/Personality-Recognition-on-RealPersonaChat" target="_blank">Code</a>
          <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="" target="_blank">Slides</a>  -->
      </div>
      
      <div class="col mt-2 p-0">
        <div id="scaling_up_llm-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            Personality recognition is useful for enhancing robots' ability to tailor user-adaptive responses, thus fostering rich human-robot interactions. One of the challenges in this task is a limited number of speakers in existing dialogue corpora, which hampers the development of robust, speaker-independent personality recognition models.
            Additionally, accurately modeling both the interdependencies among interlocutors and the intra-dependencies within the speaker in dialogues remains a significant issue.
            To address the first challenge, we introduce personality trait interpolation for speaker data augmentation. For the second, we propose heterogeneous conversational graph networks to independently capture both contextual influences and inherent personality traits. Evaluations on the RealPersonaChat corpus demonstrate our method's significant improvements over existing baselines.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li>

<li><div class="row m-0 mt-3 p-0">
  <div class="col-sm-1 p-0 abbr">
    
      
        <a class="badge font-weight-bold light-green darken-1 align-middle" style="width: 65px;" href="https://sites.google.com/grp.riken.jp/iwsds2024" target="_blank">
          IWSDS
        </a>
 
  </div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="acknowledgment" class="col p-0">
      <h5 class="title mb-0">Acknowledgment of Emotional States: Generating Validating Responses for Empathetic Dialogue</h5>
      <div class="author">
                  <nobr><a href="https://www.linkedin.com/in/zihaurpang0309/" target="_blank">Zi Haur Pang</a>,</nobr>
                  <nobr><em>Yahui Fu</em>,</nobr>
                  <nobr><a href="http://sap.ist.i.kyoto-u.ac.jp/members/lala/" target="_blank">Divesh Lala</a>,</nobr>
                  <nobr><a href="http://sap.ist.i.kyoto-u.ac.jp/members/ochi/" target="_blank">Keiko Ochi</a>,</nobr>
                  <nobr><a href="http://www.sap.ist.i.kyoto-u.ac.jp/members/inoue/" target="_blank">Koji Inoue</a>,</nobr>
                  <nobr><a href="http://sap.ist.i.kyoto-u.ac.jp/members/kawahara/" target="_blank">Tatsuya Kawahara</a>.</nobr>  
        
      </div>

      <div>
        <p class="periodical font-italic">
          
            In The 14th International Workshop on Spoken Dialogue Systems Technology, Sapporo, Japan. (Oral)
          
        </p>
      </div>
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#acknowledgment-abstract" role="button" aria-expanded="false" aria-controls="acknowledgment-abstract">Abstract</a>     
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://arxiv.org/pdf/2402.12770" target="_blank">PDF</a> 
          <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="" target="_blank">Slides</a>  -->
          <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="https://github.com/fuyahuii/Personality-Recognition-on-RealPersonaChat" target="_blank">Code</a> -->
          <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="http://arxiv.org/abs/2301.12993" target="_blank">arXiv</a>  -->
      </div>
      
      <div class="col mt-2 p-0">
        <div id="acknowledgment-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            In the realm of human-AI dialogue, the facilitation of empathetic responses is important. Validation is one of the key communication techniques in psychology, which entails recognizing, understanding, and acknowledging others' emotional states, thoughts, and actions. This study introduces a framework designed to engender empathetic dialogue with validating responses. Our approach incorporates a tripartite module system: 1) validation timing detection, 2) users' emotional state identification, and 3) validating response generation. Utilizing Japanese EmpatheticDialogues dataset - a textual-based dialogue dataset consisting of 8 emotional categories from Plutchik's wheel of emotions - the Task Adaptive Pre-Training (TAPT) BERT-based model outperforms both random baseline and the ChatGPT performance, in term of F1-score, in all modules. Further validation of our model's efficacy is confirmed in its application to the TUT Emotional Storytelling Corpus (TESC), a speech-based dialogue dataset, by surpassing both random baseline and the ChatGPT. This consistent performance across both textual and speech-based dialogues underscores the effectiveness of our framework in fostering empathetic human-AI communication.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li>
</ol>
</div>
</div>

<div class="row m-0 p-0" style="border-top: 1px solid #ddd; flex-direction: row-reverse;">
    <div class="col-sm-1 mt-2 p-0 pr-1">
      <h3 class="bibliography-year">2023</h3>
    </div>
    <div class="col-sm-11 p-0">
      <ol class="bibliography">
<li><div class="row m-0 mt-3 p-0">
  <div class="col-sm-1 p-0 abbr">
    
      
        <a class="badge font-weight-bold light-green darken-1 align-middle" style="width: 65px;" href="https://www.rsj.or.jp/en/pub/ar/about.html" target="_blank">
          <!-- AR -->
          Journal
        </a>
      
    
  </div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="obfuscation_benchmark" class="col p-0">
      <h5 class="title mb-0">Dual variational generative model and auxiliary retrieval for empathetic response generation by conversational robot.</h5>
      <div class="author">                           
                  <nobr><em>Yahui Fu</em>,</nobr>
                  <nobr><a href="http://www.sap.ist.i.kyoto-u.ac.jp/members/inoue/" target="_blank">Koji Inoue</a>,</nobr>
                  <nobr><a href="http://sap.ist.i.kyoto-u.ac.jp/members/lala/" target="_blank">Divesh Lala</a>,</nobr>
                  <nobr><a href="https://researchmap.jp/kentayamamoto29" target="_blank">Kenta Yamamoto</a>,</nobr>
                  <nobr><a href="https://researchmap.jp/chu/?lang=english" target="_blank">Chenhui Chu</a>,</nobr>
                  <nobr><a href="http://sap.ist.i.kyoto-u.ac.jp/members/kawahara/" target="_blank">Tatsuya Kawahara</a>.</nobr>                          
      </div>

      <div>
        <p class="periodical font-italic">
          
            Advanced Robotics 37 (21), 1406-1418
            2023.
          
        </p>
      </div>
    
      <div class="col p-0">       
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#obfuscation_benchmark-abstract" role="button" aria-expanded="false" aria-controls="obfuscation_benchmark-abstract">Abstract</a>
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://repository.kulib.kyoto-u.ac.jp/dspace/handle/2433/285997" target="_blank">KURENAI preprint</a>  
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/demo/20230209_Fu-system_subtitle2.mp4" target="_blank">Demo1</a>
          <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="http://arxiv.org/abs/2301.12993" target="_blank">arXiv</a> -->             
      </div>
    
      
      <div class="col mt-2 p-0">
        <div id="obfuscation_benchmark-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            Empathy in human-robot conversations aims to endow the robot with the ability to comprehend user emotion and experience, and then respond to it appropriately. Generally, empathy is embodied in the aspects of both contextual understanding and affective expression, which occur when there exist content and emotion consistencies between context and response. However, previous studies only focus on either aspect. In this paper, we propose a dual variational generative model (DVG) for empathetic response generation to achieve both. Specifically, we integrate an emotion classifier and a variational autoencoder (VAE) into a dual response and context generative model to learn the emotion and content consistencies efficiently. DVG utilizes VAE to mimic the process of context/response understanding. In addition to the generative model, our model can effectively switch to another retrieval system as a fallback solution. Automatic and human evaluations on Japanese and English EmpatheticDialogue datasets demonstrate the effectiveness of our method for empathetic response generation. Furthermore, we evaluate our model's ability in general response generation, which is not specific to empathetic but also chitchatting dialogue system.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li>

<li><div class="row m-0 mt-3 p-0">
  <div class="col-sm-1 p-0 abbr">
        <a class="badge font-weight-bold light-green darken-1 align-middle" style="width: 65px;" href="https://2023.sigdial.org/" target="_blank">
          SIGdial
        </a>  
  </div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="agile_modeling" class="col p-0">
      <h5 class="title mb-0">Reasoning before Responding: Integrating Commonsense-based Causality
        Explanation for Empathetic Response Generation.</h5>
      
        <div class="author">
                    <nobr><em>Yahui Fu</em>,</nobr>
                    <nobr><a href="http://www.sap.ist.i.kyoto-u.ac.jp/members/inoue/" target="_blank">Koji Inoue</a>,</nobr>
                    <nobr><a href="https://researchmap.jp/chu/?lang=english" target="_blank">Chenhui Chu</a>,</nobr>
                    <nobr><a href="http://sap.ist.i.kyoto-u.ac.jp/members/kawahara/" target="_blank">Tatsuya Kawahara</a>.</nobr>     
      </div>

      <div>
        <p class="periodical font-italic">
          
          In Proceedings of the 24th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 645–656, Prague, Czechia. 
          Association for Computational Linguistics. (Oral)
          
        </p>
      </div>
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#agile_modeling-abstract" role="button" aria-expanded="false" aria-controls="agile_modeling-abstract">Abstract</a>
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://aclanthology.org/2023.sigdial-1.60/" target="_blank">PDF</a>
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://arxiv.org/abs/2308.00085" target="_blank">arxiv</a>
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/SIGdial2023_Fu.pdf" target="_blank">Slides</a>
          <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="https://github.com/google-research/google-research/tree/master/agile_modeling" target="_blank">Code</a> -->
          <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="https://youtu.be/ctZuQq87tss" target="_blank">Video</a> -->
        
      </div>
      
      <div class="col mt-2 p-0">
        <div id="agile_modeling-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
          Recent approaches to empathetic response generation try to incorporate commonsense knowledge or reasoning about the causes of emotions to better understand the user's experiences and feelings. However, these approaches mainly focus on understanding the causalities of context from the user's perspective, ignoring the system's perspective. In this paper, we propose a commonsense-based causality explanation approach for diverse empathetic response generation that considers both the user's perspective (user's desires and reactions) and the system's perspective (system's intentions and reactions). We enhance ChatGPT's ability to reason for the system's perspective by integrating in-context learning with commonsense knowledge. Then, we integrate the commonsense-based causality explanation with both ChatGPT and a T5-based model. Experimental evaluations demonstrate that our method outperforms other comparable methods on both automatic and human evaluations.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li>

<li><div class="row m-0 mt-3 p-0">
  <div class="col-sm-1 p-0 abbr">
        <a class="badge font-weight-bold light-green darken-1 align-middle" style="width: 65px;" href="https://aclanthology.org/volumes/2023.yrrsds-1/" target="_blank">
          YRRSDS
        </a> 
  </div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    <div id="causality_reasoning" class="col p-0">
      <h5 class="title mb-0">Causality Reasoning for Empathy-Enriched and Personality-Conditioned Spoken Dialogue System.</h5>
      
      <div class="author">
                    <nobr><em>Yahui Fu</em></nobr>     
      </div>

      <div>
        <p class="periodical font-italic">
          
          In Proceedings of the 19th Annual Meeting of the Young Reseachers' Roundtable on Spoken Dialogue Systems, pages 62–63, Prague, Czechia. Association for Computational Linguistics.
          
        </p>
      </div>
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#causality_reasoning-abstract" role="button" aria-expanded="false" aria-controls="causality_reasoning-abstract">Abstract</a>
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://aclanthology.org/2023.yrrsds-1.23/" target="_blank">PDF</a> 
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/YRRSDS2023_Fu.pdf" target="_blank">Poster</a>
          <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="https://github.com/google-research/google-research/tree/master/agile_modeling" target="_blank">Code</a> -->
          <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="https://youtu.be/ctZuQq87tss" target="_blank">Video</a> -->
        
      </div>
      
      <div class="col mt-2 p-0">
        <div id="causality_reasoning-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            The author’s objective centers around developing a spoken dialogue system (SDS) that can emulate the cognitive and conversational qualities of a human friend. Key attributes such as empathy, knowledge/causality reasoning, and personality are integral components of human interaction. The proposed approach involves the creation of an Empathy-enriched SDS, capable of comprehending human emotions and circumstances, thus providing companionship and assistance akin to a trusted friend. Additionally, the Causality-reasoning for SDS aims to ground the system in commonsense knowledge and equip it with the ability to reason about causalities, such as predicting user desires/reactions and system intentions/reactions, thereby enhancing the system’s intelligence and human-like behavior. Finally, the concept of a Personality-conditioned SDS involves enabling systems to exhibit distinct personalities, further enhancing the naturalness of human-robot interaction.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li>

<li><div class="row m-0 mt-3 p-0">
  <div class="col-sm-1 p-0 abbr">
        <a class="badge font-weight-bold light-green darken-1 align-middle" style="width: 65px;" href="https://sites.google.com/view/iwsds2023/home" target="_blank">
          IWSDS
        </a> 
  </div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    <div id="improving_empathetic" class="col p-0">
      <h5 class="title mb-0">Improving Empathetic Response Generation with Retrieval based on Emotion Recognition.</h5>
      
      <div class="author">
                    <nobr><em>Yahui Fu</em>,</nobr>   
                    <nobr><a href="http://www.sap.ist.i.kyoto-u.ac.jp/members/inoue/" target="_blank">Koji Inoue</a>,</nobr>
                  <nobr><a href="http://sap.ist.i.kyoto-u.ac.jp/members/lala/" target="_blank">Divesh Lala</a>,</nobr>
                  <nobr><a href="https://researchmap.jp/kentayamamoto29" target="_blank">Kenta Yamamoto</a>,</nobr>
                  <nobr><a href="https://researchmap.jp/chu/?lang=english" target="_blank">Chenhui Chu</a>,</nobr>
                  <nobr><a href="http://sap.ist.i.kyoto-u.ac.jp/members/kawahara/" target="_blank">Tatsuya Kawahara</a>.</nobr>  
      </div>

      <div>
        <p class="periodical font-italic">
          
          In The 13th International Workshop on Spoken Dialogue Systems Technology, Los Angeles, USA.
          
        </p>
      </div>
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#improving_empathetic-abstract" role="button" aria-expanded="false" aria-controls="improving_empathetic-abstract">Abstract</a>
          <a class="badge grey waves-effect font-weight-light mr-1" href="http://sap.ist.i.kyoto-u.ac.jp/lab/bib/intl/FU-IWSDS23.pdf" target="_blank">PDF</a>
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/demo/20221221_Fu-system_ver2_subtitle1.mp4" target="_blank">Demo</a>
          <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/SIGdial2023_Fu.pdf" target="_blank">Slides</a> -->
          <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="https://github.com/google-research/google-research/tree/master/agile_modeling" target="_blank">Code</a> -->
          <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="https://youtu.be/ctZuQq87tss" target="_blank">Video</a> -->
        
      </div>
      
      <div class="col mt-2 p-0">
        <div id="improving_empathetic-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            Endowing a robot with the ability to express empathy is crucial for
            building a human-like dialogue system. We propose to improve empathetic response
            generation with retrieval based on emotion recognition. In addition to a generative
            model, our model can effectively switch to the retrieval system based on the context
            and emotion of the user utterances. We incorporate an emotion classifier on top
            of the context encoder and use context encoding representation to select retrieval
            responses. Furthermore, it is straightforward to combine our model with the multimodal facial expression of the virtual agent for vivid empathy. Automatic and human
            evaluations on the Japanese EmpatheticDialogue dataset demonstrate that compared
            with the solely generative model, our model can generate empathetic responses with
            more diversity and better scores on the aspects of Empathy, Relevance, and Fluency.
            Implementing our model on the autonomous android ERICA further demonstrates
            the effectiveness and adaptivity of our method in achieving an empathetic attentive listening system.
        </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li>

</ol>
</div>
</div>

<div class="row m-0 p-0" style="border-top: 1px solid #ddd; flex-direction: row-reverse;">
    <div class="col-sm-1 mt-2 p-0 pr-1">
      <h3 class="bibliography-year">2022</h3>
    </div>
    <div class="col-sm-11 p-0">
      <ol class="bibliography">
    <li><div class="row m-0 mt-3 p-0">
    <div class="col-sm-1 p-0 abbr">
    
      
        <a class="badge font-weight-bold light-green darken-1 align-middle" style="width: 65px;" href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=93" target="_blank">
          <!-- IEEE<br /><br />Multimedia -->
          Journal
        </a>
      
    
    </div>
    <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="contextandknowledge" class="col p-0">
      <h5 class="title mb-0">Context-and Knowledge-Aware Graph Convolutional Network for Multimodal Emotion Recognition.</h5>
      <div class="author">
        <nobr><em>Yahui Fu</em>,</nobr>
        <nobr><a href="https://fp.jaist.ac.jp/public/Default2.aspx?id=670&l=1" target="_blank">Shogo Okada</a>,</nobr>
        <nobr><a href="http://cic.tju.edu.cn/faculty/wanglongbiao/wang.html" target="_blank">Longbiao Wang</a>,</nobr>
        <nobr><a href="https://www.researchgate.net/profile/Lili-Guo-11" target="_blank">Lili Guo</a>,</nobr>
        <nobr>Yaodong Song,</nobr>
        <nobr><a href="https://www.researchgate.net/scientific-contributions/Jiaxing-Liu-2167637164" target="_blank">Jiaxing Liu</a>,</nobr>    
        <nobr><a href="https://researchmap.jp/jdang" target="_blank">Jianwu Dang</a>.</nobr>                    
      </div>
    </div>

      <div>
        <p class="periodical font-italic">  
          IEEE MultiMedia 29 (3), 91-100
          2022.  
        </p>
      </div>
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#contextandknowledge-abstract" role="button" aria-expanded="false" aria-controls="contextandknowledge-abstract">Abstract</a>
          <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/Context-_and_Knowledge-Aware_Graph_Convolutional_Network_for_Multimodal_Emotion_Recognition.pdf" target="_blank">PDF</a> -->
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://ieeexplore.ieee.org/document/9772497" target="_blank">PDF</a>
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://github.com/fuyahuii/ConSK-GCN" target="_blank">Code</a>
    
      </div>
    
      
      <div class="col mt-2 p-0">
        <div id="contextandknowledge-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            This work proposes an approach for emotion recognition in conversation that
            leverages context modeling, knowledge enrichment, and multimodal (text and
            audio) learning based on a graph convolutional network (GCN). We first construct
            two distinctive graphs for modeling the contextual interaction and knowledge
            dynamic. We then introduce an affective lexicon into knowledge graph building to
            enrich the emotional polarity of each concept, that is the related knowledge of each
            token in an utterance. Then, we achieve a balance between the context and the
            affect-enriched knowledge by incorporating them into the new adjacency matrix
            construction of the GCN architecture, and teach them jointly with multiple
            modalities to effectively structure the semantics-sensitive and knowledge-sensitive
            contextual dependence of each conversation. Our model outperforms the state-ofthe-art benchmarks by over 22.6% and 11% relative error reduction in terms of
            weighted-F1 on the IEMOCAP and MELD databases, respectively, demonstrating the
            superiority of our method in emotion recognition.
          </div>
        </div>
      </div>
    </div>
    </li>

    <li><div class="row m-0 mt-3 p-0">
      <div class="col-sm-1 p-0 abbr">
      
        
          <a class="badge font-weight-bold light-green darken-1 align-middle" style="width: 65px;" href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=93" target="_blank">
            <!-- IEEE<br /><br />Multimedia -->
            Journal
          </a>
        
      
      </div>
      <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
      
      <div id="emotionrecognition" class="col p-0">
        <h5 class="title mb-0">Emotion recognition with multimodal transformer fusion framework based on acoustic and lexical information.</h5>
        <div class="author">
          <nobr><a href="https://www.researchgate.net/profile/Lili-Guo-11" target="_blank">Lili Guo</a>,</nobr>
          <nobr><a href="http://cic.tju.edu.cn/faculty/wanglongbiao/wang.html" target="_blank">Longbiao Wang</a>,</nobr>
          <nobr><a href="https://researchmap.jp/jdang" target="_blank">Jianwu Dang</a>,</nobr>  
          <nobr><em>Yahui Fu</em>,</nobr>
          <nobr><a href="https://www.researchgate.net/scientific-contributions/Jiaxing-Liu-2167637164" target="_blank">Jiaxing Liu</a>,</nobr>   
          <nobr>Shifei Ding.</nobr>  
        </div>
      </div>
  
        <div>
          <p class="periodical font-italic">  
            IEEE MultiMedia 29 (2), 94-103
            2022.  
          </p>
        </div>
      
        <div class="col p-0">
          
            <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#emotionrecognition-abstract" role="button" aria-expanded="false" aria-controls="emotionrecognition-abstract">Abstract</a>
            <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/Emotion_Recognition_With_Multimodal_Transformer_Fusion_Framework_Based_on_Acoustic_and_Lexical_Information.pdf" target="_blank">PDF</a> -->
            <a class="badge grey waves-effect font-weight-light mr-1" href="https://ieeexplore.ieee.org/document/9740502" target="_blank">PDF</a>

        </div>
      
        
        <div class="col mt-2 p-0">
          <div id="emotionrecognition-abstract" class="collapse">
            <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
              People usually express emotions through paralinguistic and linguistic information in
              speech. How to effectively integrate linguistic and paralinguistic information for
              emotion recognition is a challenge. Previous studies have adopted the bidirectional
              long short-term memory (BLSTM) network to extract acoustic and lexical
              representations followed by a concatenate layer, and this has become a common
              method. However, the interaction and influence between different modalities are
              difficult to promote using simple feature fusion for each sentence. In this article, we
              propose an implicitly aligned multimodal transformer fusion (IA-MMTF) framework
              based on acoustic features and text information. This model enables the two
              modalities to guide and complement each other when learning emotional
              representations. Thereafter, the weighed fusion is used to control the contributions
              of different modalities. Thus, we can obtain more complementary emotional
              representations. Experiments on the interactive emotional dyadic motion capture
              (IEMOCAP) database and multimodal emotionlines dataset (MELD) show that the
              proposed method outperforms the baseline BLSTM-based method.
            </div>
          </div>
        </div>
      </div>
      </li>
</ol>
</div>
</div>
<!-- </div> -->

<div class="row m-0 p-0" style="border-top: 1px solid #ddd; flex-direction: row-reverse;">
    <div class="col-sm-1 mt-2 p-0 pr-1">
      <h3 class="bibliography-year">2021</h3>
    </div>
    <div class="col-sm-11 p-0">
      <ol class="bibliography">
  <li><div class="row m-0 mt-3 p-0">
  <div class="col-sm-1 p-0 abbr">
    
      
        <a class="badge font-weight-bold light-green darken-1 align-middle" style="width: 65px;" href="https://2021.ieeeicme.org/2021.ieeeicme.org/index.html" target="_blank">
          ICME
        </a>
      
    
  </div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="conskgcn" class="col p-0">
      <h5 class="title mb-0">CONSK-GCN: conversational semantic-and knowledge-oriented graph convolutional network for multimodal emotion recognition.</h5>
      <div class="author">
        <nobr><em>Yahui Fu</em>,</nobr>
        <nobr><a href="https://fp.jaist.ac.jp/public/Default2.aspx?id=670&l=1" target="_blank">Shogo Okada</a>,</nobr>
        <nobr><a href="http://cic.tju.edu.cn/faculty/wanglongbiao/wang.html" target="_blank">Longbiao Wang</a>,</nobr>
        <nobr><a href="https://www.researchgate.net/profile/Lili-Guo-11" target="_blank">Lili Guo</a>,</nobr>
        <nobr>Yaodong Song,</nobr>
        <nobr><a href="https://www.researchgate.net/scientific-contributions/Jiaxing-Liu-2167637164" target="_blank">Jiaxing Liu</a>,</nobr>    
        <nobr><a href="https://researchmap.jp/jdang" target="_blank">Jianwu Dang</a>.</nobr>   
      </div>

      <div>
        <p class="periodical font-italic">
          
          In 2021 IEEE International Conference on Multimedia and Expo (ICME) (pp. 1-6). IEEE. (Oral)
          
        </p>
      </div>
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#conskgcn-abstract" role="button" aria-expanded="false" aria-controls="conskgcn-abstract">Abstract</a>
        
        
        
          <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/CONSK-GCN_Conversational_Semantic-_and_Knowledge-Oriented_Graph_Convolutional_Network_for_Multimodal_Emotion_Recognition.pdf" target="_blank">PDF</a> -->
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://ieeexplore.ieee.org/document/9772497" target="_blank">PDF</a>
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://github.com/fuyahuii/ConSK-GCN" target="_blank">Code</a>
        
      </div>
    
      
      <div class="col mt-2 p-0">
        <div id="conskgcn-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            Emotion recognition in conversations (ERC) has received significant attention in recent years due to its widespread applications in diverse areas, such as social media, health care, and artificial intelligence interactions. However, different from nonconversational text, it is particularly challenging to model the effective context-aware dependence for the task of ERC. To address this problem, we propose a new Conversational Semantic- and Knowledge-oriented Graph Convolutional Network (ConSK-GCN) approach that leverages both semantic dependence and commonsense knowledge. First, we construct the contextual inter-interaction and intradependence of the interlocutors via a conversational graph-based convolutional network based on multimodal representations. Second, we incorporate commonsense knowledge to guide ConSK-GCN to model the semantic-sensitive and knowledge-sensitive contextual dependence. The results of extensive experiments show that the proposed method outperforms the current state of the art on the IEMOCAP dataset.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li>
<li><div class="row m-0 mt-3 p-0">
  <div class="col-sm-1 p-0 abbr">
    
      
        <a class="badge font-weight-bold light-green darken-1 align-middle" style="width: 65px;" href="https://www.2021.ieeeicassp.org/2021.ieeeicassp.org/index.html" target="_blank">
          ICASSP
        </a>
      
    
  </div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="multimodal" class="col p-0">
      <h5 class="title mb-0">Multimodal emotion recognition with capsule graph convolutional based representation fusion.</h5>
      <div class="author">
        <nobr><a href="https://www.researchgate.net/scientific-contributions/Jiaxing-Liu-2167637164" target="_blank">Jiaxing Liu</a>,</nobr>   
        <nobr><a href="https://www.researchgate.net/scientific-contributions/Sen-Chen-2196003227" target="_blank">Sen Chen</a>,</nobr>
        <nobr><a href="http://cic.tju.edu.cn/faculty/wanglongbiao/wang.html" target="_blank">Longbiao Wang</a>,</nobr>
        <nobr><a href="http://cic.tju.edu.cn/faculty/zhileiliu/index.html" target="_blank">Zhilei Liu</a>,</nobr>
        <nobr><em>Yahui Fu</em>,</nobr>
        <nobr><a href="https://www.researchgate.net/profile/Lili-Guo-11" target="_blank">Lili Guo</a>,</nobr>    
        <nobr><a href="https://researchmap.jp/jdang" target="_blank">Jianwu Dang</a>.</nobr>   
      </div>

      <div>
        <p class="periodical font-italic">
          
          In 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 6339-6343. IEEE.
          
        </p>
      </div>
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#multimodal-abstract" role="button" aria-expanded="false" aria-controls="multimodal-abstract">Abstract</a>
          <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/Multimodal_Emotion_Recognition_with_Capsule_Graph_Convolutional_Based_Representation_Fusion.pdf" target="_blank">PDF</a> -->
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://ieeexplore.ieee.org/document/9413608" target="_blank">PDF</a>
          <!-- <a class="badge grey waves-effect font-weight-light mr-1" href="https://github.com/fuyahuii/ConSK-GCN" target="_blank">Code</a> -->
        
      </div>
    
      
      <div class="col mt-2 p-0">
        <div id="multimodal-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            Due to the more robust characteristics compared to unimodal, audio-video multimodal emotion recognition (MER) has attracted a lot of attention. The efficiency of representation fusion algorithm often determines the performance of MER. Although there are many fusion algorithms, information redundancy and information complementarity are usually ignored. In this paper, we propose a novel representation fusion method, Capsule Graph Convolutional Network (CapsGCN). Firstly, after unimodal representation learning, the extracted audio and video representations are distilled by capsule network and encapsulated into multimodal capsules respectively. Multimodal capsules can effectively reduce data redundancy by the dynamic routing algorithm. Secondly, the multimodal capsules with their inter-relations and intra-relations are treated as a graph structure. The graph structure is learned by Graph Convolutional Network (GCN) to get hidden representation which is a good supplement for information complementarity. Finally, the multimodal capsules and hidden relational representation learned by CapsGCN are fed to multihead self-attention to balance the contributions of source representation and relational representation. To verify the performance, visualization of representation, the results of commonly used fusion methods, and ablation studies of the proposed CapsGCN are provided. Our proposed fusion method achieves 80.83% accuracy and 80.23% F1 score on eNTERFACE05’.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li>
<li><div class="row m-0 mt-3 p-0">
  <div class="col-sm-1 p-0 abbr">
    
      
        <a class="badge font-weight-bold light-green darken-1 align-middle" style="width: 65px;" href="https://link.springer.com/book/10.1007/978-3-030-67832-6" target="_blank">
          MMM
        </a>
      
    
  </div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="asentiment" class="col p-0">
      <h5 class="title mb-0">A sentiment similarity-oriented attention model with multi-task learning for text-based emotion recognition.</h5>
      <div class="author">
        <nobr><em>Yahui Fu</em>,</nobr>
        <nobr><a href="https://www.researchgate.net/profile/Lili-Guo-11" target="_blank">Lili Guo</a>,</nobr>
        <nobr><a href="http://cic.tju.edu.cn/faculty/wanglongbiao/wang.html" target="_blank">Longbiao Wang</a>,</nobr>
        <nobr><a href="http://cic.tju.edu.cn/faculty/zhileiliu/index.html" target="_blank">Zhilei Liu</a>,</nobr>
        <nobr><a href="https://www.researchgate.net/scientific-contributions/Jiaxing-Liu-2167637164" target="_blank">Jiaxing Liu</a>,</nobr> 
        <nobr><a href="https://researchmap.jp/jdang" target="_blank">Jianwu Dang</a>.</nobr>   
      </div>

      <div>
        <p class="periodical font-italic">
          
          In MultiMedia Modeling: 27th International Conference, MMM 2021, Prague, Czech Republic, June 22–24, 2021, Proceedings, Part I 27 (pp. 278-289). Springer International Publishing.
          
        </p>
      </div>
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#asentiment-abstract" role="button" aria-expanded="false" aria-controls="asentiment-abstract">Abstract</a>
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://link.springer.com/chapter/10.1007/978-3-030-67832-6_23" target="_blank">PDF</a>
      </div>
    
      
      <div class="col mt-2 p-0">
        <div id="asentiment-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            Emotion recognition based on text modality has been one of the major topics in
            the field of emotion recognition in conversation. How to extract efficient emotional features
            is still a challenge. Previous studies utilize contextual semantics and emotion lexicon for af-
            fect modeling. However, they ignore information that may be conveyed by the emotion labels
            themselves. To address this problem, we propose the sentiment similarity-oriented attention
            (SSOA) mechanism, which uses the semantics of emotion labels to guide the model’s attention
            when encoding the input conversations. Thus to extract emotion-related information from sen-
            tences. Then we use the convolutional neural network (CNN) to extract complex informative
            features. In addition, as discrete emotions are highly related with the Valence, Arousal, and
            Dominance (VAD) in psychophysiology, we train the VAD regression and emotion classifica-
            tion tasks together by using multi-task learning to extract more robust features. The proposed
            method outperforms the benchmarks by an absolute increase of over 3.65% in terms of the
            average F1 for the emotion classification task, and also outperforms previous strategies for the
            VAD regression task on the IEMOCAP database
            </div>  
        </div>
      </div>
      
    </div>
  </div>
</div>
</li></ol>
    </div>
  </div>


  </div>

  <!-- Footer -->
  <footer>
    &copy; Copyright 2023 Yahui Fu.
    
    
  </footer>

  <!-- Core JavaScript Files -->
  <script src="/assets/js/jquery.min.js" type="text/javascript"></script>
  <script src="/assets/js/popper.min.js" type="text/javascript"></script>
  <script src="/assets/js/bootstrap.min.js" type="text/javascript"></script>
  <script src="/assets/js/mdb.min.js" type="text/javascript"></script>
  <script async="" src="https://cdnjs.cloudflare.com/ajax/libs/masonry/4.2.2/masonry.pkgd.min.js" integrity="sha384-GNFwBvfVxBkLMJpYMOABq3c+d3KnQxudP/mGPkzpZSTYykLBNsZEnG2D9G/X/+7D" crossorigin="anonymous"></script>
  <script src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML"></script>
  <script src="/assets/js/common.js"></script>

  <!-- Scrolling Progress Bar -->
  <script type="text/javascript">
    $(document).ready(function() {
      var navbarHeight = $('#navbar').outerHeight(true);
      $('body').css({ 'padding-top': navbarHeight });
      $('progress-container').css({ 'padding-top': navbarHeight });
      var progressBar = $('#progress');
      progressBar.css({ 'top': navbarHeight });
      var getMax = function() { return $(document).height() - $(window).height(); }
      var getValue = function() { return $(window).scrollTop(); }   
      // Check if the browser supports the progress element.
      if ('max' in document.createElement('progress')) {
        // Set the 'max' attribute for the first time.
        progressBar.attr({ max: getMax() });
        progressBar.attr({ value: getValue() });
    
        $(document).on('scroll', function() {
          // On scroll only the 'value' attribute needs to be calculated.
          progressBar.attr({ value: getValue() });
        });

        $(window).resize(function() {
          var navbarHeight = $('#navbar').outerHeight(true);
          $('body').css({ 'padding-top': navbarHeight });
          $('progress-container').css({ 'padding-top': navbarHeight });
          progressBar.css({ 'top': navbarHeight });
          // On resize, both the 'max' and 'value' attributes need to be calculated.
          progressBar.attr({ max: getMax(), value: getValue() });
        });
      } else {
        var max = getMax(), value, width;
        var getWidth = function() {
          // Calculate the window width as a percentage.
          value = getValue();
          width = (value/max) * 100;
          width = width + '%';
          return width;
        }
        var setWidth = function() { progressBar.css({ width: getWidth() }); };
        setWidth();
        $(document).on('scroll', setWidth);
        $(window).on('resize', function() {
          // Need to reset the 'max' attribute.
          max = getMax();
          setWidth();
        });
      }
    });
  </script>

  <!-- Code Syntax Highlighting -->
  <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet">
  <script src="/assets/js/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

  <!-- Script Used for Randomizing the Projects Order -->
  <!-- <script type="text/javascript">
    $.fn.shuffleChildren = function() {
      $.each(this.get(), function(index, el) {
        var $el = $(el);
        var $find = $el.children();

        $find.sort(function() {
          return 0.5 - Math.random();
        });

        $el.empty();
        $find.appendTo($el);
      });
    };
    $("#projects").shuffleChildren();
  </script> -->

  <!-- Project Cards Layout -->
  <script type="text/javascript">
    var $grid = $('#projects');

    // $grid.masonry({ percentPosition: true });
    // $grid.masonry('layout');

    // Trigger after images load.
    $grid.imagesLoaded().progress(function() {
      $grid.masonry({ percentPosition: true });
      $grid.masonry('layout');
    });
  </script>

  <!-- Enable Tooltips -->
  <script type="text/javascript">
    $(function () {
      $('[data-toggle="tooltip"]').tooltip()
    })
  </script>

  <!-- Google Analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', '', 'auto');
    ga('send', 'pageview');
  </script>
</body>
</html>
